{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "def initialise_session():\n",
    "    K.clear_session()\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    num_physical_devices = len(physical_devices)\n",
    "    print(\"GPUs Available: \", num_physical_devices)\n",
    "    if num_physical_devices > 0:\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        \n",
    "def find_directory():\n",
    "    try:\n",
    "        # Check if running in a Jupyter notebook\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            # Running in a Jupyter notebook\n",
    "            directory = os.getcwd()\n",
    "        else:\n",
    "            # Running in a standalone Python script\n",
    "            directory = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # Running in a standalone Python script\n",
    "        directory = Path(__file__).parent\n",
    "    return directory\n",
    "            \n",
    "def build_train_image_paths(path_to_data):\n",
    "    def absolute_file_paths(directory):\n",
    "        for dirpath, _, filenames in os.walk(directory):\n",
    "            for filename in sorted(filenames, key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else float('inf')):\n",
    "                if not filename.split('.')[0].isdigit():\n",
    "                    continue\n",
    "                yield os.path.abspath(os.path.join(dirpath, filename))\n",
    "    train_image_paths = []\n",
    "    for file_path in absolute_file_paths(f\"{path_to_data}/training_data/combined\"):\n",
    "        train_image_paths.append(file_path)\n",
    "    path_indexes_to_remove = []\n",
    "    for i, image_path in enumerate(train_image_paths):\n",
    "        try:\n",
    "            Image.open(image_path)\n",
    "        except UnidentifiedImageError:\n",
    "            path_indexes_to_remove.append(i)\n",
    "    for index in sorted(path_indexes_to_remove, reverse=True):\n",
    "        train_image_paths.pop(index)\n",
    "        \n",
    "    train_image_paths_and_labels = pd.DataFrame()\n",
    "    train_image_paths_and_labels['image_path'] = train_image_paths\n",
    "\n",
    "    training_norm = pd.read_csv(f\"{path_to_data}/training_norm.csv\")\n",
    "    for image_path in train_image_paths:\n",
    "        image_id = int((image_path.split('.')[0]).split('/')[-1])\n",
    "        angle = training_norm.loc[training_norm['image_id'] == image_id,\n",
    "                                'angle'].iloc[0]\n",
    "        speed = int((training_norm.loc[training_norm['image_id'] == image_id,\n",
    "                                    'speed'].iloc)[0])\n",
    "        train_image_paths_and_labels.loc[train_image_paths_and_labels['image_path'] == image_path, 'angle'] = angle\n",
    "        train_image_paths_and_labels.loc[train_image_paths_and_labels['image_path'] == image_path, 'speed'] = speed\n",
    "    return train_image_paths_and_labels\n",
    "\n",
    "def build_test_image_paths(path_to_data):\n",
    "    image_ids = np.arange(1, 1021)\n",
    "    test_image_paths = pd.DataFrame({'image_path': []})\n",
    "    for image_id in image_ids:\n",
    "        full_path = str(f\"{path_to_data}/test_data/test_data/{str(image_id)}.png\")\n",
    "        test_image_paths.loc[len(test_image_paths)] = full_path\n",
    "    return test_image_paths\n",
    "        \n",
    "def build_image_paths(directory):\n",
    "    path_to_data = f\"{directory}/machine-learning-in-science-ii-2023\"\n",
    "    train_image_paths_and_labels = build_train_image_paths(path_to_data)\n",
    "    test_image_paths = build_test_image_paths(path_to_data)\n",
    "    return train_image_paths_and_labels, test_image_paths\n",
    "\n",
    "def load_files_and_paths(directory):\n",
    "    if 'google.colab' in sys.modules:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        path_to_data = Path(\"/content/drive/MyDrive/machine-learning-in-science-ii-2023\")\n",
    "        train_image_paths = pd.read_csv(str(path_to_data/'training_norm_paths_googledrive.csv'))\n",
    "        test_image_paths = pd.read_csv(str(path_to_data/'test_image_paths_googledrive.csv'))\n",
    "    else:\n",
    "        train_image_paths, test_image_paths = build_image_paths(directory)\n",
    "    return train_image_paths, test_image_paths\n",
    "    \n",
    "def f1_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate the F1 score.\n",
    "    \"\"\"\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Recall metric.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Precision metric.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    return f1_score\n",
    "\n",
    "def load_training_images_and_labels(image_path, speed_label, angle_label, image_shape):\n",
    "    # Define a function that maps each row to an image and a pair of labels\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, image_shape)\n",
    "    speed_label = tf.cast(speed_label, tf.int32)\n",
    "    angle_label = tf.cast(angle_label, tf.float32)\n",
    "    return image, (speed_label, angle_label)\n",
    "\n",
    "def augment(image_label, seed):\n",
    "    image, label = image_label\n",
    "    new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "        image, max_delta=0.5, seed=new_seed)\n",
    "    return image, label\n",
    "\n",
    "def build_training_and_validation_sets(train_image_paths, image_shape, batch_size, train_val_split):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_image_paths['image_path'],\n",
    "                                                  train_image_paths['speed'],\n",
    "                                                  train_image_paths['angle']))\n",
    "    dataset = dataset.map(lambda x,y,z: load_training_images_and_labels(x,y,z, image_shape)).batch(batch_size)\n",
    "    num_batches = dataset.cardinality().numpy()\n",
    "    dataset = dataset.shuffle(buffer_size=num_batches*batch_size)\n",
    "\n",
    "    train_batches = int(num_batches * train_val_split)\n",
    "    val_batches = num_batches - train_batches\n",
    "\n",
    "    train_set = dataset.take(train_batches)\n",
    "    val_set = dataset.skip(train_batches)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    counter = tf.data.Dataset.counter()\n",
    "    train_set = tf.data.Dataset.zip((train_set, (counter, counter)))\n",
    "    train_set = train_set.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    train_set = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_set = val_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "class CNNs(keras.Model):\n",
    "    def __init__(self, image_shape, name='CNNs'):\n",
    "        super(CNNs, self).__init__(name=name)\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        self.CNN_speed = keras.Sequential([\n",
    "            Input(shape=image_shape+(3,)),\n",
    "            layers.Conv2D(32, 3,\n",
    "                          padding=\"valid\",\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            #layers.Dropout(0.5),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            #layers.Dropout(0.5),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            #layers.Dropout(0.5),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=keras.regularizers.l2(0.075)\n",
    "                         ),\n",
    "            #layers.Dropout(0.5),\n",
    "            layers.Dense(10)\n",
    "        ], name='CNN_speed')\n",
    "        \n",
    "        self.CNN_angle = keras.Sequential([\n",
    "            Input(shape=image_shape+(3,)),\n",
    "            layers.Conv2D(32, 3, padding=\"valid\", activation=\"relu\"),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3, activation=\"relu\"),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation=\"relu\"),\n",
    "            layers.Dense(10)\n",
    "        ], name='CNN_angle')\n",
    "\n",
    "        self.speed_output = keras.layers.Dense(1, activation=None, name='speed')\n",
    "        self.angle_output = keras.layers.Dense(1, activation='linear', name='angle')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.CNN_speed(inputs)\n",
    "        y = self.CNN_angle(inputs)\n",
    "        \n",
    "        speed_output = self.speed_output(x)\n",
    "        angle_output = self.angle_output(y)\n",
    "        return [speed_output, angle_output]\n",
    "\n",
    "\n",
    "def build_model(image_shape):\n",
    "    inputs = keras.layers.Input(shape=image_shape+(3,))\n",
    "    [speed_output, angle_output] = CNNs(image_shape)(inputs)\n",
    "\n",
    "    # Name outputs\n",
    "    speed_output = layers.Lambda(lambda x: x, name='speed')(speed_output)\n",
    "    angle_output = layers.Lambda(lambda x: x, name='angle')(angle_output)\n",
    "\n",
    "    model = keras.models.Model(inputs=inputs, outputs=[speed_output, angle_output])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_set, val_set, epochs, logging):\n",
    "    model.compile(\n",
    "        optimizer = keras.mixed_precision.LossScaleOptimizer(Adam()),\n",
    "        loss={\n",
    "            'speed': keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "            'angle': keras.losses.MeanSquaredError()\n",
    "        },\n",
    "        loss_weights={\n",
    "            'speed': 1,\n",
    "            'angle': 1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    callback = keras.callbacks.EarlyStopping(monitor='val_speed_loss',\n",
    "                                                start_from_epoch=1000,\n",
    "                                                restore_best_weights=True)\n",
    "\n",
    "    if logging:\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tf.profiler.experimental.start(log_dir)\n",
    "    \n",
    "    history = model.fit(train_set,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_set,\n",
    "                        callbacks=[callback])\n",
    "    \n",
    "    if logging:\n",
    "        tf.profiler.experimental.stop()\n",
    "        # to view log execute: \"tensorboard --logdir=logs/fit/\"\n",
    "    return model, history\n",
    "\n",
    "def plot_training_curve(history, epochs, epoch_offset):\n",
    "    lowest_speed_loss = str(round(min(history.history['speed_loss']), 5))\n",
    "    print(f\"Lowest speed loss: {lowest_speed_loss}\")\n",
    "    lowest_val_speed_loss = str(round(min(history.history['val_speed_loss']), 5))\n",
    "    print(f\"Lowest validation speed loss: {lowest_val_speed_loss}\")\n",
    "    loss = history.history['speed_loss']\n",
    "    val_loss = history.history['val_speed_loss']\n",
    "    epochs_range = range(epochs)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range[epoch_offset:], loss[epoch_offset:], label='Training Loss')\n",
    "    plt.plot(epochs_range[epoch_offset:], val_loss[epoch_offset:], label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Speed Loss')\n",
    "    plt.show()\n",
    "    return lowest_val_speed_loss\n",
    "    \n",
    "def load_testing_images(image_path, image_shape):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, image_shape)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "def build_test_set(test_image_paths, image_shape):\n",
    "    test_set = tf.data.Dataset.from_tensor_slices(test_image_paths['image_path'])\n",
    "    test_set = test_set.map(lambda x: load_testing_images(x, image_shape))\n",
    "    return test_set\n",
    "\n",
    "def threshold_predictions(predictions_df):\n",
    "    boundary = lambda x: 1 if x > 0.5 else 0\n",
    "    predictions_df['speed'] = predictions_df['speed'].apply(boundary)\n",
    "\n",
    "    angles = [0.0, 0.0625, 0.125, 0.1875, 0.25, 0.3125, 0.375, 0.4375, 0.5, 0.5625,\n",
    "            0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0]\n",
    "    closest_angle_round = lambda x: angles[min(range(len(angles)),\n",
    "                                            key = lambda i: abs(angles[i]-x))]\n",
    "    predictions_df['angle'] = predictions_df['angle'].apply(closest_angle_round)\n",
    "    return predictions_df\n",
    "\n",
    "def make_predictions(model, test_set):\n",
    "    predictions = model.predict(test_set)\n",
    "\n",
    "    speed_predictions = predictions[0]\n",
    "    angle_predictions = predictions[1]\n",
    "\n",
    "    predictions_df = pd.DataFrame()\n",
    "    predictions_df['image_id'] = np.arange(1, 1021)\n",
    "    predictions_df['angle'] = angle_predictions\n",
    "    predictions_df['speed'] = speed_predictions\n",
    "\n",
    "    predictions_df = threshold_predictions(predictions_df)\n",
    "    return predictions_df\n",
    "    \n",
    "def create_submission(predictions_df, name):\n",
    "    predictions_df.to_csv(f\"submissions/submission_speedval-{name}.csv\", index=False)\n",
    "    \n",
    "def save_tf_model(model, name, path_to_models):\n",
    "    tf_save_path = str(path_to_models/f\"tf/{name}/\")\n",
    "    tf.saved_model.save(model, tf_save_path)\n",
    "    return tf_save_path\n",
    "\n",
    "def save_tflite_model(model, name, path_to_models, tf_save_path):\n",
    "    # Convert the model to tflite\n",
    "    tflite_model = tf.lite.TFLiteConverter.from_saved_model(tf_save_path).convert()\n",
    "\n",
    "    tflite_save_path = path_to_models/f\"tflite/{name}\"\n",
    "    os.mkdir(tflite_save_path)\n",
    "    with open(tflite_save_path/'model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "        \n",
    "def test_model(model, test_image_paths, image_shape, name):\n",
    "    test_decision = None\n",
    "    print('Make submission? (y/n)')\n",
    "    while test_decision != 'y' and test_decision != 'n':\n",
    "        test_decision = input('>>> ')\n",
    "        if test_decision == 'y':\n",
    "            test_set = build_test_set(test_image_paths, image_shape)\n",
    "            predictions_df = make_predictions(model, test_set)\n",
    "            create_submission(predictions_df, name)\n",
    "        elif test_decision == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid. Enter 'y' or 'n'\")\n",
    "\n",
    "def save_model(model, name, directory):\n",
    "    save_decision = None\n",
    "    print('Save model? (y/n)')\n",
    "    while save_decision != 'y' and save_decision != 'n':\n",
    "        save_decision = input('>>> ')\n",
    "        if save_decision == 'y':\n",
    "            path_to_models = directory/'models'\n",
    "            tf_save_path = save_tf_model(model, name, path_to_models)\n",
    "            save_tflite_model(model, name, path_to_models, tf_save_path)\n",
    "        elif save_decision == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid. Enter 'y' or 'n'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "batch_size = 40\n",
    "epochs = 6\n",
    "train_val_split = 0.8\n",
    "image_shape = (32, 32) # (32, 32) works well\n",
    "logging = False # log using tensorboard\n",
    "# -----------------------\n",
    "\n",
    "# --- TRAIN ---\n",
    "initialise_session()\n",
    "directory = find_directory()\n",
    "train_image_paths, test_image_paths = load_files_and_paths(directory)\n",
    "train_set, val_set = build_training_and_validation_sets(train_image_paths,\n",
    "                                                        image_shape,\n",
    "                                                        batch_size,\n",
    "                                                        train_val_split)\n",
    "model = build_model(image_shape)\n",
    "model, history = train_model(model, train_set, val_set, epochs, logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSE ---\n",
    "name = plot_training_curve(history, epochs, epoch_offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAKE SUBMISSION ---\n",
    "test_model(model, test_image_paths, image_shape, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SAVE MODEL ---\n",
    "save_model(model, name, directory)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
