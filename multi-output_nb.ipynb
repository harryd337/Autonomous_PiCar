{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import UnidentifiedImageError\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def initialise_session():\n",
    "    K.clear_session()\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    num_physical_devices = len(physical_devices)\n",
    "    print(\"GPUs Available: \", num_physical_devices)\n",
    "    if num_physical_devices > 0:\n",
    "        tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        \n",
    "def find_directory():\n",
    "    try:\n",
    "        # Check if running in a Jupyter notebook\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            # Running in a Jupyter notebook\n",
    "            directory = os.getcwd()\n",
    "        else:\n",
    "            # Running in a standalone Python script\n",
    "            directory = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # Running in a standalone Python script\n",
    "        directory = Path(__file__).parent\n",
    "    return directory\n",
    "            \n",
    "def build_train_image_paths(path_to_data):\n",
    "    def absolute_file_paths(directory):\n",
    "        for dirpath, _, filenames in os.walk(directory):\n",
    "            for filename in sorted(filenames, key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else float('inf')):\n",
    "                if not filename.split('.')[0].isdigit():\n",
    "                    continue\n",
    "                yield os.path.abspath(os.path.join(dirpath, filename))\n",
    "    train_image_paths = []\n",
    "    for file_path in absolute_file_paths(f\"{path_to_data}/training_data/combined\"):\n",
    "        train_image_paths.append(file_path)\n",
    "    path_indexes_to_remove = []\n",
    "    for i, image_path in enumerate(train_image_paths):\n",
    "        try:\n",
    "            Image.open(image_path)\n",
    "        except UnidentifiedImageError:\n",
    "            path_indexes_to_remove.append(i)\n",
    "    for index in sorted(path_indexes_to_remove, reverse=True):\n",
    "        train_image_paths.pop(index)\n",
    "        \n",
    "    train_image_paths_and_labels = pd.DataFrame()\n",
    "    train_image_paths_and_labels['image_path'] = train_image_paths\n",
    "\n",
    "    training_norm = pd.read_csv(f\"{path_to_data}/training_norm.csv\")\n",
    "    for image_path in train_image_paths:\n",
    "        image_id = int((image_path.split('.')[0]).split('/')[-1])\n",
    "        angle = training_norm.loc[training_norm['image_id'] == image_id,\n",
    "                                'angle'].iloc[0]\n",
    "        speed = int((training_norm.loc[training_norm['image_id'] == image_id,\n",
    "                                    'speed'].iloc)[0])\n",
    "        train_image_paths_and_labels.loc[train_image_paths_and_labels['image_path'] == image_path, 'angle'] = angle\n",
    "        train_image_paths_and_labels.loc[train_image_paths_and_labels['image_path'] == image_path, 'speed'] = speed\n",
    "    train_image_paths_0 = train_image_paths_and_labels[train_image_paths_and_labels['speed'] == 0].reset_index(drop=True)\n",
    "    train_image_paths_1 = train_image_paths_and_labels[train_image_paths_and_labels['speed'] == 1].reset_index(drop=True)\n",
    "    return train_image_paths_0, train_image_paths_1\n",
    "\n",
    "def build_test_image_paths(path_to_data):\n",
    "    image_ids = np.arange(1, 1021)\n",
    "    test_image_paths = pd.DataFrame({'image_path': []})\n",
    "    for image_id in image_ids:\n",
    "        full_path = str(f\"{path_to_data}/test_data/test_data/{str(image_id)}.png\")\n",
    "        test_image_paths.loc[len(test_image_paths)] = full_path\n",
    "    return test_image_paths\n",
    "        \n",
    "def build_image_paths(directory):\n",
    "    path_to_data = f\"{directory}/machine-learning-in-science-ii-2023\"\n",
    "    train_image_paths_0, train_image_paths_1 = build_train_image_paths(path_to_data)\n",
    "    test_image_paths = build_test_image_paths(path_to_data)\n",
    "    return train_image_paths_0, train_image_paths_1, test_image_paths\n",
    "\n",
    "def load_files_and_paths(directory):\n",
    "    if 'google.colab' in sys.modules:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        path_to_data = Path(\"/content/drive/MyDrive/machine-learning-in-science-ii-2023\")\n",
    "        train_image_paths = pd.read_csv(str(path_to_data/'training_norm_paths_googledrive.csv'))\n",
    "        test_image_paths = pd.read_csv(str(path_to_data/'test_image_paths_googledrive.csv'))\n",
    "    else:\n",
    "        train_image_paths_0, train_image_paths_1, test_image_paths = build_image_paths(directory)\n",
    "    return train_image_paths_0, train_image_paths_1, test_image_paths\n",
    "    \n",
    "def load_training_images_and_labels(image_path, labels, weights, image_shape):\n",
    "    # Define a function that maps each row to an image and a pair of labels\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, image_shape)\n",
    "    speed_label = labels[0]\n",
    "    angle_label = labels[1]\n",
    "    speed_label = tf.cast(speed_label, tf.int32)\n",
    "    angle_label = tf.cast(angle_label, tf.float32)\n",
    "    return image, (speed_label, angle_label), weights\n",
    "\n",
    "def augment(train_set, seed):\n",
    "    image, labels, weights = train_set\n",
    "    new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "        image, max_delta=0.5, seed=new_seed)\n",
    "    return image, labels, weights\n",
    "\n",
    "def build_training_validation_and_evaluation_sets(train_image_paths_0, train_image_paths_1, image_shape, batch_size, eval_split, train_val_split, speed_weights):\n",
    "    speed_0_weights = np.ones(train_image_paths_0.shape[0]) * speed_weights[0]\n",
    "    speed_1_weights = np.ones(train_image_paths_1.shape[0]) * speed_weights[1]\n",
    "    dataset_0 = tf.data.Dataset.from_tensor_slices((train_image_paths_0['image_path'], \n",
    "                                                  (train_image_paths_0['speed'], train_image_paths_0['angle']),\n",
    "                                                  (speed_0_weights, None)\n",
    "                                                  ))\n",
    "    dataset_1 = tf.data.Dataset.from_tensor_slices((train_image_paths_1['image_path'], \n",
    "                                                  (train_image_paths_1['speed'], train_image_paths_1['angle']),\n",
    "                                                  (speed_1_weights, None)\n",
    "                                                  ))\n",
    "\n",
    "    dataset_0 = dataset_0.map(lambda x,y,z: load_training_images_and_labels(x,y,z, image_shape))\n",
    "    dataset_1 = dataset_1.map(lambda x,y,z: load_training_images_and_labels(x,y,z, image_shape))\n",
    "    \n",
    "    dataset_0_size = dataset_0.cardinality().numpy()\n",
    "    dataset_1_size = dataset_1.cardinality().numpy()\n",
    "    num_images = dataset_0_size + dataset_1_size\n",
    "    \n",
    "    eval_0_size = int(dataset_0.cardinality().numpy()*eval_split)\n",
    "    eval_1_size = int(dataset_1.cardinality().numpy()*eval_split)\n",
    "    \n",
    "    if eval_0_size <= eval_1_size:\n",
    "        eval_half_size = eval_0_size\n",
    "        less_zeros = True\n",
    "        extra_ones = eval_1_size - eval_0_size\n",
    "    else:\n",
    "        eval_half_size = eval_1_size\n",
    "        less_zeros = False\n",
    "        \n",
    "    dataset_0 = dataset_0.shuffle(buffer_size=dataset_0_size)\n",
    "    dataset_1 = dataset_1.shuffle(buffer_size=dataset_1_size)\n",
    "    dataset_0 = dataset_0.batch(eval_half_size)\n",
    "    dataset_1 = dataset_1.batch(eval_half_size)\n",
    "    \n",
    "    eval_set_0s = dataset_0.take(1)\n",
    "    eval_set_1s = dataset_1.take(1)\n",
    "    \n",
    "    eval_set_speed_size = eval_half_size*2\n",
    "    eval_set_speed = eval_set_0s.concatenate(eval_set_1s).unbatch().shuffle(buffer_size=eval_set_speed_size).batch(batch_size)\n",
    "    \n",
    "    dataset_0 = dataset_0.skip(1).take(-1).unbatch()\n",
    "    dataset_1 = dataset_1.skip(1).take(-1).unbatch()\n",
    "    \n",
    "    if less_zeros:\n",
    "        dataset_1 = dataset_1.batch(extra_ones)\n",
    "        eval_set_angle_size = eval_1_size\n",
    "        eval_set_angle = dataset_1.take(1).concatenate(eval_set_1s).unbatch().shuffle(buffer_size=eval_set_angle_size).batch(batch_size)\n",
    "        dataset_1 = dataset_1.skip(1).take(-1).unbatch()\n",
    "    else:\n",
    "        eval_set_angle_size = eval_half_size\n",
    "        eval_set_angle = eval_set_1s.unbatch().shuffle(buffer_size=eval_set_angle_size).batch(batch_size)\n",
    "    \n",
    "    eval_set_speed_percentage = int(round((eval_set_speed_size / num_images) * 100, 0))\n",
    "    eval_set_angle_percentage = int(round((eval_set_angle_size / num_images) * 100, 0))\n",
    "    \n",
    "    num_0s = dataset_0.reduce(0, lambda x,_: x+1).numpy()\n",
    "    num_1s = dataset_1.reduce(0, lambda x,_: x+1).numpy()\n",
    "    speed_split = f\"{int(round((num_0s / (num_0s + num_1s))*100, 0))}/{int(round((num_1s / (num_0s + num_1s))*100, 0))}\"\n",
    "    dataset = dataset_0.concatenate(dataset_1)\n",
    "    \n",
    "    num_train_val_images = dataset.reduce(0, lambda x,_: x+1).numpy() \n",
    "    dataset = dataset.shuffle(buffer_size=num_train_val_images).batch(batch_size)\n",
    "    num_batches = dataset.reduce(0, lambda x,_: x+1).numpy()\n",
    "\n",
    "    train_batches = int(num_batches * train_val_split[0])\n",
    "    val_batches = int(num_batches * train_val_split[1])\n",
    "\n",
    "    train_set = dataset.take(train_batches)\n",
    "    val_set = dataset.skip(train_batches)\n",
    "\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    counter = tf.data.Dataset.counter()\n",
    "    train_set = tf.data.Dataset.zip((train_set, (counter, counter)))\n",
    "    train_set = train_set.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    train_set = train_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_set = val_set.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    eval_set_speed = eval_set_speed.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    eval_set_angle = eval_set_angle.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    if (num_train_val_images % batch_size) != 0:\n",
    "        num_full_batches = int(np.floor(num_train_val_images / batch_size))\n",
    "        last_batch_size = num_train_val_images % num_full_batches\n",
    "    else:\n",
    "        last_batch_size = batch_size\n",
    "        \n",
    "    train_set_size = train_set.reduce(0, lambda x,_: x+1).numpy() * batch_size\n",
    "    train_set_percentage = int(round((train_set_size / num_images) * 100, 0))\n",
    "    \n",
    "    val_set_size = (val_set.reduce(0, lambda x,_: x+1).numpy() - 1) * batch_size + last_batch_size\n",
    "    val_set_percentage = int(round((val_set_size / num_images) * 100, 0))\n",
    "    \n",
    "    print(f\"\\nFound {num_images} images.\")\n",
    "    print(f\"Using {train_set_size} ({train_set_percentage}%) for training (speed split {speed_split}).\")\n",
    "    print(f\"Using {val_set_size} ({val_set_percentage}%) for validation.\")\n",
    "    print(f\"Using {eval_set_speed_size} ({eval_set_speed_percentage}%) for evaluation of speed (speed split 50/50).\")\n",
    "    print(f\"Using {eval_set_angle_size} ({eval_set_angle_percentage}%) for evaluation of angle.\\n\")\n",
    "    \n",
    "    return train_set, val_set, eval_set_speed, eval_set_angle\n",
    "\n",
    "def build_model(image_shape):\n",
    "    inputs = keras.layers.Input(shape=image_shape+(3,))\n",
    "    [speed_output, angle_output] = CNNs(image_shape)(inputs)\n",
    "\n",
    "    # Name outputs\n",
    "    speed_output = layers.Lambda(lambda x: x, name='speed')(speed_output)\n",
    "    angle_output = layers.Lambda(lambda x: x, name='angle')(angle_output)\n",
    "\n",
    "    model = keras.models.Model(inputs=inputs, outputs=[speed_output, angle_output])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "       \n",
    "class EvaluationCallback(Callback):\n",
    "    def __init__(self, eval_set_speed, eval_set_angle, speed_threshold, f_score_beta):\n",
    "        super().__init__()\n",
    "        self.eval_set_speed = eval_set_speed\n",
    "        self.eval_set_angle = eval_set_angle\n",
    "        self.combined_mse_history = []\n",
    "        self.mse_speed_history = []\n",
    "        self.mse_angle_history = []\n",
    "        self.f_score_negative_history = []\n",
    "        self.specificity_history = []\n",
    "        self.NPV_history = []\n",
    "        # Get the true labels for the evaluation sets\n",
    "        self.true_speeds = np.concatenate([y[0].numpy() for x, y, z in self.eval_set_speed], axis=0)\n",
    "        self.true_angles = np.concatenate([y[1].numpy() for x, y, z in self.eval_set_angle], axis=0)\n",
    "        self.apply_speed_threshold = lambda x: 1 if x > speed_threshold else 0\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        # Assuming `eval_set` is the evaluation dataset returned by your function\n",
    "        # and `model` is your trained model\n",
    "\n",
    "        # Run model.predict on the evaluation set to get the predicted labels\n",
    "        predictions = model.predict(self.eval_set_speed, verbose=0)\n",
    "        speed_predictions = predictions[0]\n",
    "        \n",
    "        predictions = model.predict(self.eval_set_angle, verbose=0)\n",
    "        angle_predictions = predictions[1]\n",
    "        \n",
    "        speed_predictions = np.vectorize(self.apply_speed_threshold)(speed_predictions)\n",
    "        NPV, specificity, f_score_negative, _ = precision_recall_fscore_support(self.true_speeds,\n",
    "                                                                       speed_predictions,\n",
    "                                                                       beta=f_score_beta,\n",
    "                                                                       pos_label=0,\n",
    "                                                                       average='binary',\n",
    "                                                                       zero_division=0)\n",
    "\n",
    "        # Calculate the MSE for each output separately\n",
    "        mse_speed = mean_squared_error(self.true_speeds, speed_predictions)\n",
    "        mse_angle = mean_squared_error(self.true_angles, angle_predictions)\n",
    "\n",
    "        # Combine the MSE values into a single metric\n",
    "        combined_mse = mse_speed + mse_angle\n",
    "        \n",
    "        self.combined_mse_history.append(combined_mse)\n",
    "        self.mse_speed_history.append(mse_speed)\n",
    "        self.mse_angle_history.append(mse_angle)\n",
    "        self.f_score_negative_history.append(f_score_negative)\n",
    "        self.specificity_history.append(specificity)\n",
    "        self.NPV_history.append(NPV)\n",
    "        print(f\"\\nEval_MSE: {round(combined_mse, 4)} - Eval_Speed_MSE: \\\n",
    "{round(mse_speed, 4)} - Eval_Angle_MSE: {round(float(mse_angle), 4)}\")\n",
    "        print(f\"Eval_Speed_F{f_score_beta}-negative: {round(f_score_negative, 4)}\\\n",
    " - Eval_Speed_Specificity: {round(specificity, 4)} - Eval_Speed_NPV: {round(NPV, 4)}\")\n",
    "        \n",
    "class Specificity(tf.keras.metrics.Metric):\n",
    "    def __init__(self, speed_threshold, name='specificity', **kwargs):\n",
    "        super(Specificity, self).__init__(name=name, **kwargs)\n",
    "        self.tn = tf.keras.metrics.TrueNegatives(speed_threshold)\n",
    "        self.fp = tf.keras.metrics.FalsePositives(speed_threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.tn.update_state(y_true, y_pred)\n",
    "        self.fp.update_state(y_true, y_pred)\n",
    "\n",
    "    def result(self):\n",
    "        return self.tn.result() / (self.tn.result() + self.fp.result())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.tn.reset_state()\n",
    "        self.fp.reset_state()\n",
    "        \n",
    "class NPV(tf.keras.metrics.Metric):\n",
    "    def __init__(self, speed_threshold, name='NPV', **kwargs):\n",
    "        super(NPV, self).__init__(name=name, **kwargs)\n",
    "        self.tn = tf.keras.metrics.TrueNegatives(speed_threshold)\n",
    "        self.fn = tf.keras.metrics.FalseNegatives(speed_threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.tn.update_state(y_true, y_pred)\n",
    "        self.fn.update_state(y_true, y_pred)\n",
    "\n",
    "    def result(self):\n",
    "        return self.tn.result() / (self.tn.result() + self.fn.result())\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.tn.reset_state()\n",
    "        self.fn.reset_state()\n",
    "        \n",
    "class F_score_negative(tf.keras.metrics.Metric):\n",
    "    def __init__(self, speed_threshold, f_score_beta, name='F_score_negative', **kwargs):\n",
    "        super(F_score_negative, self).__init__(name=name, **kwargs)\n",
    "        self.tn = tf.keras.metrics.TrueNegatives(speed_threshold)\n",
    "        self.fn = tf.keras.metrics.FalseNegatives(speed_threshold)\n",
    "        self.fp = tf.keras.metrics.FalsePositives(speed_threshold)\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.tn.update_state(y_true, y_pred)\n",
    "        self.fn.update_state(y_true, y_pred)\n",
    "        self.fp.update_state(y_true, y_pred)\n",
    "\n",
    "    def result(self):\n",
    "        NPV = self.tn.result() / (self.tn.result() + self.fn.result())\n",
    "        specificity = self.tn.result() / (self.tn.result() + self.fp.result())\n",
    "        return ((1 + f_score_beta**2) * (NPV * specificity)) / ((f_score_beta**2) * NPV + specificity)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.tn.reset_state()\n",
    "        self.fn.reset_state()\n",
    "        self.fp.reset_state()\n",
    "\n",
    "def create_learning_rate_schedule(initial_learning_rate, final_learning_rate, total_decay_steps):\n",
    "    decay_rate = (final_learning_rate / initial_learning_rate) ** (1. / total_decay_steps)\n",
    "    learning_rate_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=total_decay_steps,\n",
    "        decay_rate=decay_rate,\n",
    "        staircase=True)\n",
    "    return learning_rate_schedule\n",
    "\n",
    "def compile_model(model, initial_learning_rate, final_learning_rate, total_decay_steps, speed_threshold, f_score_beta):\n",
    "    learning_rate_schedule = create_learning_rate_schedule(initial_learning_rate, final_learning_rate, total_decay_steps)\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate=learning_rate_schedule),\n",
    "        loss={\n",
    "            'speed': keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "            'angle': keras.losses.MeanSquaredError()\n",
    "        },\n",
    "        metrics={\n",
    "        'speed': [F_score_negative(speed_threshold, f_score_beta), Specificity(speed_threshold), NPV(speed_threshold)]\n",
    "        },\n",
    "        loss_weights={\n",
    "            'speed': 1,\n",
    "            'angle': 1\n",
    "        },\n",
    "        weighted_metrics = []\n",
    "    )\n",
    "    return model\n",
    "        \n",
    "def train_model(model, train_set, val_set, eval_set_speed, eval_set_angle, epochs, logging, speed_threshold, f_score_beta):\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                                            start_from_epoch=1000,\n",
    "                                                            restore_best_weights=True)\n",
    "    evaluation_callback = EvaluationCallback(eval_set_speed, eval_set_angle, speed_threshold, f_score_beta)\n",
    "\n",
    "    if logging:\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tf.profiler.experimental.start(log_dir)\n",
    "    \n",
    "    history = model.fit(train_set,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=val_set,\n",
    "                        callbacks=[early_stopping_callback, evaluation_callback])\n",
    "    \n",
    "    if logging:\n",
    "        tf.profiler.experimental.stop()\n",
    "        # to view log execute: \"tensorboard --logdir=logs/fit/\"\n",
    "    return model, history, evaluation_callback\n",
    "\n",
    "def evaluate_model(history, evaluation_callback, f_score_beta, epochs, epoch_offset):\n",
    "    combined_mse_history = evaluation_callback.combined_mse_history\n",
    "    mse_speed_history = evaluation_callback.mse_speed_history\n",
    "    mse_angle_history = evaluation_callback.mse_angle_history\n",
    "    f_score_negative_history = evaluation_callback.f_score_negative_history\n",
    "    specificity_history = evaluation_callback.specificity_history\n",
    "    NPV_history = evaluation_callback.NPV_history\n",
    "    lowest_evaluation_loss = str(round(min(combined_mse_history), 5))\n",
    "    print(f\"Lowest evaluation loss: {lowest_evaluation_loss}\")\n",
    "    print(f\"Highest evaluation F-score: {round(max(f_score_negative_history), 3)}\")\n",
    "    speed_loss = history.history['speed_loss']\n",
    "    val_speed_loss = history.history['val_speed_loss']\n",
    "    angle_loss = history.history['angle_loss']\n",
    "    val_angle_loss = history.history['val_angle_loss']\n",
    "    epochs_range = range(epochs)\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 15))\n",
    "    ax1, ax2, ax3, ax4 = axs.ravel()\n",
    "    \n",
    "    ax1.plot(epochs_range[epoch_offset:], combined_mse_history[epoch_offset:], color='g', label='Evaluation MSE')\n",
    "    ax1.plot(epochs_range[epoch_offset:], mse_speed_history[epoch_offset:], color='r', label='Speed MSE')\n",
    "    ax1.plot(epochs_range[epoch_offset:], mse_angle_history[epoch_offset:], color='purple', label='Angle MSE')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.set_title('Evaluation Loss')\n",
    "    \n",
    "    ax2.plot(epochs_range[epoch_offset:], f_score_negative_history[epoch_offset:], color='black', label=f\"F{f_score_beta}-negative\")\n",
    "    ax2.plot(epochs_range[epoch_offset:], specificity_history[epoch_offset:], color='grey', label='Specificity')\n",
    "    #ax2.plot(epochs_range[epoch_offset:], NPV_history[epoch_offset:], color='brown', label='NPV')\n",
    "    ax2.legend(loc='upper right')\n",
    "    ax2.set_title(f\"Evaluation of Speed\")\n",
    "    \n",
    "    ax3.plot(epochs_range[epoch_offset:], speed_loss[epoch_offset:], label='Speed Training Loss')\n",
    "    ax3.plot(epochs_range[epoch_offset:], val_speed_loss[epoch_offset:], label='Speed Validation Loss')\n",
    "    ax3.legend(loc='upper right')\n",
    "    ax3.set_title('Speed Training and Validation Loss')\n",
    "    \n",
    "    ax4.plot(epochs_range[epoch_offset:], angle_loss[epoch_offset:], label='Angle Training Loss')\n",
    "    ax4.plot(epochs_range[epoch_offset:], val_angle_loss[epoch_offset:], label='Angle Validation Loss')\n",
    "    ax4.legend(loc='upper right')\n",
    "    ax4.set_title('Angle Training and Validation Loss')\n",
    "    \n",
    "    plt.show()\n",
    "    return lowest_evaluation_loss\n",
    "    \n",
    "def load_testing_images(image_path, image_shape):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, image_shape)\n",
    "    return image\n",
    "\n",
    "def build_test_set(test_image_paths, image_shape, batch_size):\n",
    "    test_set = tf.data.Dataset.from_tensor_slices(test_image_paths['image_path'])\n",
    "    test_set = test_set.map(lambda x: load_testing_images(x, image_shape)).batch(batch_size)\n",
    "    test_set = test_set.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return test_set\n",
    "\n",
    "def threshold_predictions(speed_predictions, angle_predictions, speed_threshold):\n",
    "    apply_speed_threshold = lambda x: 1 if x > speed_threshold else 0\n",
    "    angles = [0.0, 0.0625, 0.125, 0.1875, 0.25, 0.3125, 0.375, 0.4375, 0.5, \n",
    "              0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0]\n",
    "    apply_angle_threshold = lambda x: angles[min(range(len(angles)),\n",
    "                                                key = lambda i: abs(angles[i]-x))]\n",
    "    speed_predictions = np.vectorize(apply_speed_threshold)(speed_predictions)\n",
    "    angle_predictions = np.vectorize(apply_angle_threshold)(angle_predictions)\n",
    "    return speed_predictions, angle_predictions\n",
    "\n",
    "def make_predictions(model, test_set, speed_threshold):\n",
    "    predictions = model.predict(test_set)\n",
    "    speed_predictions = predictions[0]\n",
    "    angle_predictions = predictions[1]\n",
    "    \n",
    "    speed_predictions, angle_predictions = threshold_predictions(speed_predictions, angle_predictions, speed_threshold)\n",
    "\n",
    "    predictions_df = pd.DataFrame()\n",
    "    predictions_df['image_id'] = np.arange(1, 1021)\n",
    "    predictions_df['angle'] = angle_predictions\n",
    "    predictions_df['speed'] = speed_predictions\n",
    "    return predictions_df\n",
    "    \n",
    "def create_submission(predictions_df, name, directory):\n",
    "    submission_directory = f\"{directory}/submissions\"\n",
    "    if not os.path.exists(submission_directory):\n",
    "        os.makedirs(submission_directory)\n",
    "    predictions_df.to_csv(f\"{submission_directory}/submission_eval-{name}.csv\", index=False)\n",
    "    \n",
    "def save_tf_model(model, name, path_to_models):\n",
    "    tf_model_save_path = f\"{path_to_models}/tf/{name}/\"\n",
    "    tf.saved_model.save(model, tf_model_save_path)\n",
    "    return tf_model_save_path\n",
    "\n",
    "def save_tflite_model(model, name, path_to_models, tf_model_save_path):\n",
    "    # Convert the model to tflite\n",
    "    tflite_model = tf.lite.TFLiteConverter.from_saved_model(tf_model_save_path).convert()\n",
    "    tflite_save_path = f\"{path_to_models}/tflite\"\n",
    "    if not os.path.exists(tflite_save_path):\n",
    "        os.makedirs(tflite_save_path)\n",
    "    # tflite_model_save_path = f\"{tflite_save_path}/{name}\"\n",
    "    # os.mkdir(tflite_model_save_path)\n",
    "    with open(f\"{tflite_save_path}/{name}.tflite\", 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "def test_model(model, test_image_paths, image_shape, batch_size, speed_threshold, name, directory):\n",
    "    test_decision = None\n",
    "    print('Make submission? (y/n)')\n",
    "    while test_decision != 'y' and test_decision != 'n':\n",
    "        test_decision = input('>>> ')\n",
    "        if test_decision == 'y':\n",
    "            test_set = build_test_set(test_image_paths, image_shape, batch_size)\n",
    "            predictions_df = make_predictions(model, test_set, speed_threshold)\n",
    "            create_submission(predictions_df, name, directory)\n",
    "        elif test_decision == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid. Enter 'y' or 'n'\")\n",
    "\n",
    "def save_model(model, name, directory):\n",
    "    save_decision = None\n",
    "    print('Save model? (y/n)')\n",
    "    while save_decision != 'y' and save_decision != 'n':\n",
    "        save_decision = input('>>> ')\n",
    "        if save_decision == 'y':\n",
    "            path_to_models = f\"{directory}/models\"\n",
    "            if not os.path.exists(path_to_models):\n",
    "                os.makedirs(path_to_models)\n",
    "            tf_model_save_path = save_tf_model(model, name, path_to_models)\n",
    "            save_tflite_model(model, name, path_to_models, tf_model_save_path)\n",
    "        elif save_decision == 'n':\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid. Enter 'y' or 'n'\")\n",
    "            \n",
    "class CNNs(keras.Model):\n",
    "    def __init__(self, image_shape, name='CNNs'):\n",
    "        super(CNNs, self).__init__(name=name)\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        self.CNN_speed = keras.Sequential([\n",
    "            Input(shape=image_shape+(3,)),\n",
    "            layers.Conv2D(32, 3,\n",
    "                          padding=\"valid\",\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=keras.regularizers.l2(0.7)\n",
    "                         ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10,\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.7)\n",
    "                        ),\n",
    "            layers.Dropout(0.5)\n",
    "        ], name='CNN_speed')\n",
    "        \n",
    "        self.CNN_angle = keras.Sequential([\n",
    "            Input(shape=image_shape+(3,)),\n",
    "            layers.Conv2D(32, 3,\n",
    "                          padding=\"valid\",\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(64, 3,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.MaxPooling2D(),\n",
    "            layers.Conv2D(128, 3,\n",
    "                          activation=\"relu\",\n",
    "                          kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                          ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64,\n",
    "                         activation=\"relu\",\n",
    "                         kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                         ),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10,\n",
    "                        kernel_regularizer=keras.regularizers.l2(0.01)\n",
    "                        ),\n",
    "            layers.Dropout(0.5)\n",
    "        ], name='CNN_angle')\n",
    "\n",
    "        self.speed_output = keras.layers.Dense(1, activation='sigmoid', name='speed')\n",
    "        self.angle_output = keras.layers.Dense(1, activation='linear', name='angle')\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.CNN_speed(inputs)\n",
    "        y = self.CNN_angle(inputs)\n",
    "        \n",
    "        speed_output = self.speed_output(x)\n",
    "        angle_output = self.angle_output(y)\n",
    "        return [speed_output, angle_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "batch_size = 40\n",
    "epochs = 200\n",
    "eval_split = 0.1\n",
    "train_val_split = [0.7, 0.3]\n",
    "image_shape = (32, 32)\n",
    "logging = False # log using tensorboard\n",
    "initial_learning_rate = 0.001\n",
    "final_learning_rate = 0.0001\n",
    "total_decay_steps = 200\n",
    "f_score_beta = 1 # F1 score\n",
    "speed_threshold = 0.5\n",
    "speed_weights = [3, 1]\n",
    "# -----------------------\n",
    "\n",
    "# --- TRAIN ---\n",
    "initialise_session()\n",
    "directory = find_directory()\n",
    "train_image_paths_0, train_image_paths_1, test_image_paths = load_files_and_paths(directory)\n",
    "train_set, val_set, eval_set_speed, eval_set_angle = build_training_validation_and_evaluation_sets(train_image_paths_0, train_image_paths_1,\n",
    "                                                                   image_shape,\n",
    "                                                                   batch_size,\n",
    "                                                                    eval_split,\n",
    "                                                                   train_val_split,\n",
    "                                                                   speed_weights)\n",
    "model = build_model(image_shape)\n",
    "model = compile_model(model,\n",
    "                      initial_learning_rate,\n",
    "                      final_learning_rate,\n",
    "                      total_decay_steps,\n",
    "                     speed_threshold,\n",
    "                     f_score_beta)\n",
    "model, history, evaluation_callback = train_model(model,\n",
    "                                                     train_set,\n",
    "                                                     val_set,\n",
    "                                                     eval_set_speed,\n",
    "                                                     eval_set_angle,\n",
    "                                                     epochs,\n",
    "                                                     logging,\n",
    "                                                     speed_threshold,\n",
    "                                                    f_score_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ANALYSE ---\n",
    "name = evaluate_model(history, evaluation_callback, f_score_beta, epochs, epoch_offset=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make submission? (y/n)\n",
      ">>> y\n",
      "26/26 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# --- MAKE SUBMISSION ---\n",
    "test_model(model, test_image_paths, image_shape, batch_size, speed_threshold, name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model? (y/n)\n",
      ">>> y\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(10, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb02340>, 139621013294736), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb141c0>, 139621945827456), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(10, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb1cfa0>, 139621945828336), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb32e20>, 139621945828736), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(10, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb02340>, 139621013294736), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb141c0>, 139621945827456), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(10, 1), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb1cfa0>, 139621945828336), {}).\n",
      "INFO:tensorflow:Unsupported signature for serialization: ((TensorSpec(shape=(1,), dtype=tf.float32, name='gradient'), <tensorflow.python.framework.func_graph.UnknownArgument object at 0x7efc0cb32e20>, 139621945828736), {}).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ppyhd1/DeepDreams/models/tf/0.02401/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/ppyhd1/DeepDreams/models/tf/0.02401/assets\n"
     ]
    }
   ],
   "source": [
    "# --- SAVE MODEL ---\n",
    "save_model(model, name, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_directory():\n",
    "    try:\n",
    "        # Check if running in a Jupyter notebook\n",
    "        shell = get_ipython().__class__.__name__\n",
    "        if shell == 'ZMQInteractiveShell':\n",
    "            # Running in a Jupyter notebook\n",
    "            directory = os.getcwd()\n",
    "        else:\n",
    "            # Running in a standalone Python script\n",
    "            directory = Path(__file__).parent\n",
    "    except NameError:\n",
    "        # Running in a standalone Python script\n",
    "        directory = Path(__file__).parent\n",
    "    return directory\n",
    "\n",
    "def build_train_image_paths(path_to_data):\n",
    "    def absolute_file_paths(directory):\n",
    "        for dirpath, _, filenames in os.walk(directory):\n",
    "            for filename in sorted(filenames, key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else float('inf')):\n",
    "                if not filename.split('.')[0].isdigit():\n",
    "                    continue\n",
    "                yield os.path.abspath(os.path.join(dirpath, filename))\n",
    "    train_image_paths = []\n",
    "    for file_path in absolute_file_paths(f\"{path_to_data}/training_data/combined\"):\n",
    "        train_image_paths.append(file_path)\n",
    "    path_indexes_to_remove = []\n",
    "    for i, image_path in enumerate(train_image_paths):\n",
    "        try:\n",
    "            Image.open(image_path)\n",
    "        except UnidentifiedImageError:\n",
    "            path_indexes_to_remove.append(i)\n",
    "    for index in sorted(path_indexes_to_remove, reverse=True):\n",
    "        train_image_paths.pop(index)\n",
    "        \n",
    "    train_image_paths_labels = pd.DataFrame()\n",
    "    train_image_paths_labels['image_path'] = train_image_paths\n",
    "\n",
    "    training_norm = pd.read_csv(f\"{path_to_data}/training_norm.csv\")\n",
    "    for image_path in train_image_paths:\n",
    "        image_id = int((image_path.split('.')[0]).split('/')[-1])\n",
    "        angle = training_norm.loc[training_norm['image_id'] == image_id,\n",
    "                                'angle'].iloc[0]\n",
    "        speed = int((training_norm.loc[training_norm['image_id'] == image_id,\n",
    "                                    'speed'].iloc)[0])\n",
    "        train_image_paths_labels.loc[train_image_paths_labels['image_path'] == image_path, 'angle'] = angle\n",
    "        train_image_paths_labels.loc[train_image_paths_labels['image_path'] == image_path, 'speed'] = speed\n",
    "    train_image_paths_0 = train_image_paths_labels[train_image_paths_labels['speed'] == 0].reset_index(drop=True).drop('speed', axis=1)\n",
    "    train_image_paths_1 = train_image_paths_labels[train_image_paths_labels['speed'] == 1].reset_index(drop=True).drop('speed', axis=1)\n",
    "    return train_image_paths_0, train_image_paths_1\n",
    "\n",
    "directory = find_directory()\n",
    "path_to_data = f\"{directory}/machine-learning-in-science-ii-2023\"\n",
    "train_image_paths_0, train_image_paths_1 = build_train_image_paths(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
